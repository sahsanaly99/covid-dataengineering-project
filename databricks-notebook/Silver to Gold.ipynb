{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "456b87cc-e8a5-4925-8331-ace21764df7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Securely configure Spark to access Azure Data Lake Storage Gen2 (ADLS Gen2)\n",
    "# using OAuth 2.0 with Azure Active Directory via service principal credentials.\n",
    "# Secrets like the client secret are retrieved securely from Databricks secret scope.\n",
    "\n",
    "# Set the client secret using Databricks secrets for secure storage\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.covid2025storage.dfs.core.windows.net\",\n",
    "               dbutils.secrets.get(scope=\"myscope\", key=\"sp-secret\"))\n",
    "\n",
    "# Set the authentication type to OAuth for accessing the storage account\n",
    "spark.conf.set(\"fs.azure.account.auth.type.covid2025storage.dfs.core.windows.net\", \"OAuth\")\n",
    "\n",
    "# Specify the provider class that supports OAuth 2.0 client credentials flow\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.covid2025storage.dfs.core.windows.net\", \n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "\n",
    "# Set the client ID (application ID) of your Azure AD registered app\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.covid2025storage.dfs.core.windows.net\", \n",
    "               \"af9d0047-d4cd-4d32-81a0-3e2ebeff1343\")\n",
    "               \n",
    "# Define the token endpoint for your Azure tenant to obtain access tokens\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.covid2025storage.dfs.core.windows.net\", \n",
    "               \"https://login.microsoftonline.com/fdae273f-8a49-469c-b9bc-444ce8f28607/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39294146-40bd-4cff-b4d5-5130863f6931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries to read and manipulate the notebook\n",
    "df_silver = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"abfss://silver@covid2025storage.dfs.core.windows.net/countrywise.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ec2b9d-f3c9-426b-a442-07aaf0e4390c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b197de5-aad7-4eeb-979b-ae1bc6abb856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "df_gold = df_silver.withColumn(\"mortality_rate\", round((col(\"Deaths\") / col(\"Confirmed\")) * 100, 2)) \\\n",
    ".withColumn(\"recovery_rate\", round((col(\"Recovered\") / col(\"Confirmed\")) * 100, 2)) \\\n",
    ".withColumn(\"weekly_growth_rate\", round((col(\"one_week_change\") / col(\"confirmed_last_week\")) * 100, 2)) \\\n",
    ".withColumn(\"active_ratio\", round((col(\"Active\") / col(\"Confirmed\")), 2))\n",
    "\n",
    "\n",
    "df_gold.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6b1aa9-bf7b-46bc-b8d4-7dc85e0709d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, sum\n",
    "\n",
    "df_gold_region = df_gold.groupBy(\"who_region\").agg(\n",
    "    sum(\"Confirmed\").alias(\"total_confirmed\"),\n",
    "    sum(\"Deaths\").alias(\"total_deaths\"),\n",
    "    sum(\"Recovered\").alias(\"total_recovered\"),\n",
    "    sum(\"Active\").alias(\"total_active\"),\n",
    "    round(avg(\"mortality_rate\"),2).alias(\"avg_mortality_rate\"),\n",
    "    round(avg(\"recovery_rate\"),2).alias(\"avg_recovery_rate\"),\n",
    "    round(avg(\"weekly_growth_rate\"),2).alias(\"avg_weekly_growth_rate\")\n",
    ")\n",
    "\n",
    "df_gold_region.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b4a0ec-04b0-41bd-8cd9-943cf1d067f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_dfs = [\n",
    "    (df_gold, \"countrywise\"),\n",
    "    (df_gold_region, \"regionwise\")\n",
    "]\n",
    "\n",
    "for df, name in gold_dfs:\n",
    "    temp_path = f\"abfss://gold@covid2025storage.dfs.core.windows.net/temp_{name}/\"\n",
    "    \n",
    "    df.repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_path)\n",
    "\n",
    "    files = dbutils.fs.ls(temp_path)\n",
    "    for file in files:\n",
    "        if file.name.endswith(\".csv\"):\n",
    "            part_file_path = file.path\n",
    "            break\n",
    "\n",
    "    final_output_path = f\"abfss://gold@covid2025storage.dfs.core.windows.net/{name}.csv\"\n",
    "\n",
    "    dbutils.fs.mv(part_file_path, final_output_path)\n",
    "    dbutils.fs.rm(temp_path, True)\n",
    "\n",
    "    display(dbutils.fs.ls(\"abfss://gold@covid2025storage.dfs.core.windows.net/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02ca237-ffc7-4de3-b2f2-5e64a29ef082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver to Gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
